Embeddings map discrete tokens into continuous vectors.

It can be used after a tokenizer


Pytorch functional form. That way you don't have to call a module for it.


logits.permute


Automated training:

Train it on math problems.


Keep adding to it without losing the current knowledge. (Can you add structure without training it from scratch each time?)

TODO:
Stack multiple transformers
